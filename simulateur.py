"""
Swarm robot simulator (Gym-compatible) - single file
Features:
- 2D world with obstacles
- Multiple differential-drive robots (3 by default)
- Simple LIDAR (raycasts)
- Wi-Fi simulator (latency & packet loss)
- Domain randomization + noise
- Gym-compatible Env (reset/step/render)
- Pygame renderer for visualization
- Example: run with random policy or use stable-baselines3 if installed

How to use:
- pip install gym pygame numpy
- (optionally) pip install stable-baselines3[extra]
- python simulator_swarm_env.py --mode demo

Author: generated by ChatGPT for 'robot essaims'
"""

import os
import math
import random
import argparse
import time
from dataclasses import dataclass
from typing import List, Tuple, Dict, Optional
import numpy as np

# ----------------------------- Pygame init ---------------------------------
try:
    import pygame
except Exception:
    pygame = None

def init_pygame_if_needed():
    if pygame is None:
        return
    if not pygame.get_init():
        pygame.init()
    if not pygame.display.get_init():
        pygame.display.init()

try:
    import gym
    from gym import spaces
except Exception:
    # Minimal shim for types only; training libs expect gym.Env interface
    class spaces:
        class Box:
            def __init__(self, low, high, shape=None, dtype=np.float32):
                pass
        class Discrete:
            def __init__(self, n):
                pass

    class gym:
        Env = object

# ----------------------------- Config & Helpers -----------------------------

@dataclass
class Config:
    world_w: int = 800
    world_h: int = 600
    n_robots: int = 3
    robot_radius: float = 12.0
    max_speed: float = 80.0             # pixels per second
    max_omega: float = math.radians(90) # rad/s
    dt: float = 0.1                     # step time in seconds
    lidar_range: float = 150.0          # pixels
    lidar_beams: int = 16
    wifi_range: float = 300.0           # pixels
    wifi_loss_prob: float = 0.03        # default packet loss
    wifi_latency_mean: float = 0.05     # seconds
    wifi_latency_std: float = 0.02
    domain_randomization: bool = True
    collision_penalty: float = -5.0
    goal_reward: float = 10.0
    wall_penalty: float = -1.0

# ----------------------------- Physics primitives ---------------------------

@dataclass
class Obstacle:
    x: float
    y: float
    w: float
    h: float

    def contains(self, px, py):
        return (self.x <= px <= self.x + self.w) and (self.y <= py <= self.y + self.h)

# ----------------------------- Wi-Fi simulator ------------------------------

class WifiSimulator:
    """Simple model: checks if two nodes are in range, then applies packet loss and latency.
    Messages are modeled as instantaneous for the simulator (we only use it to optionally drop actions/observations).
    """
    def __init__(self, cfg: Config):
        self.cfg = cfg

    def transmit(self, sender_pos, receiver_pos, payload=None) -> Tuple[bool, float]:
        dist = math.hypot(sender_pos[0] - receiver_pos[0], sender_pos[1] - receiver_pos[1])
        if dist > self.cfg.wifi_range:
            return False, float('inf')
        # packet loss
        if random.random() < self.cfg.wifi_loss_prob:
            return False, float('inf')
        # latency
        latency = max(0.0, random.gauss(self.cfg.wifi_latency_mean, self.cfg.wifi_latency_std))
        return True, latency

# ----------------------------- Robot model ---------------------------------

class Robot:
    def __init__(self, x, y, theta, cfg: Config, robot_id:int=0):
        self.x = float(x)
        self.y = float(y)
        self.theta = float(theta)
        self.v = 0.0
        self.w = 0.0
        self.cfg = cfg
        self.id = robot_id

    def step(self, v_cmd, w_cmd, dt, noise=True):
        # Clip
        v = float(np.clip(v_cmd, -self.cfg.max_speed, self.cfg.max_speed))
        w = float(np.clip(w_cmd, -self.cfg.max_omega, self.cfg.max_omega))
        # actuation noise
        if noise:
            v *= random.uniform(0.9, 1.1)
            w *= random.uniform(0.9, 1.1)
        # integrate
        self.x += v * math.cos(self.theta) * dt
        self.y += v * math.sin(self.theta) * dt
        self.theta += w * dt
        # normalize theta
        self.theta = (self.theta + math.pi) % (2 * math.pi) - math.pi
        self.v = v
        self.w = w

    def pos(self):
        return (self.x, self.y)

# ----------------------------- Environment ---------------------------------

class SwarmEnv(gym.Env):
    """Gym-compatible multi-robot environment. Observation is a flattened vector for all robots.
    Action: for each robot, continuous (v, omega).
    Observation: for each robot: [x, y, theta, v, lidar_beams...]
    """
    metadata = {'render.modes': ['human']}

    def __init__(self, cfg: Optional[Config] = None):
        self.cfg = cfg or Config()
        self.robots: List[Robot] = []
        self.obstacles: List[Obstacle] = []
        self.goals: List[Tuple[float,float]] = []
        self.wifi = WifiSimulator(self.cfg)
        self.time = 0.0
        self._init_spaces()
        self._build_default_world()
        self.viewer = None
        self.screen = None
        self.clock = None

    def _init_spaces(self):
        # action: (n_robots, 2) continuous
        act_low = np.array([-self.cfg.max_speed, -self.cfg.max_omega], dtype=np.float32)
        act_high = np.array([self.cfg.max_speed, self.cfg.max_omega], dtype=np.float32)
        self.action_space = spaces.Box(low=np.tile(act_low, self.cfg.n_robots),
                                       high=np.tile(act_high, self.cfg.n_robots), dtype=np.float32)

        # obs: per robot: x,y,theta,v + lidar beams
        obs_per = 4 + self.cfg.lidar_beams
        low = np.full(obs_per * self.cfg.n_robots, -np.inf, dtype=np.float32)
        high = np.full(obs_per * self.cfg.n_robots, np.inf, dtype=np.float32)
        self.observation_space = spaces.Box(low=low, high=high, dtype=np.float32)

    def _build_default_world(self):
        W, H = self.cfg.world_w, self.cfg.world_h
        # obstacles: some rectangles
        self.obstacles = [
            Obstacle(W*0.2, H*0.3, 80, 200),
            Obstacle(W*0.6, H*0.1, 120, 120),
            Obstacle(W*0.5, H*0.6, 200, 60),
        ]
        # goals: one per robot or shared target
        self.goals = [(W*0.8, H*0.8) for _ in range(self.cfg.n_robots)]

    def reset(self):
        self.time = 0.0
        self.robots = []
        margin = 60
        for i in range(self.cfg.n_robots):
            while True:
                x = random.uniform(margin, self.cfg.world_w - margin)
                y = random.uniform(margin, self.cfg.world_h - margin)
                # avoid spawning inside obstacles
                if any(ob.contains(x,y) for ob in self.obstacles):
                    continue
                self.robots.append(Robot(x, y, random.uniform(-math.pi, math.pi), self.cfg, robot_id=i))
                break
        # domain randomization
        self._apply_domain_randomization()
        return self._get_obs()

    def _apply_domain_randomization(self):
        if not self.cfg.domain_randomization:
            return
        # randomize wifi parameters, motor noise range, lidar noise, etc.
        self.cfg.wifi_loss_prob = random.uniform(0.0, 0.1)
        self.cfg.wifi_latency_mean = random.uniform(0.0, 0.12)
        # small variations to speeds and sensors
        self.cfg.max_speed *= random.uniform(0.9, 1.1)
        self.cfg.max_omega *= random.uniform(0.9, 1.1)

    def step(self, action):
        """action: flat array len = n_robots*2 -> [(v,w) per robot]
        returns obs, reward, done, info
        """
        self.time += self.cfg.dt
        action = np.asarray(action, dtype=np.float32).reshape(self.cfg.n_robots, 2)
        infos = {'wifi': []}
        # apply actions with wifi effects
        for i, robot in enumerate(self.robots):
            v_cmd, w_cmd = action[i]
            # optionally drop action due to wifi
            ok, latency = self.wifi.transmit(robot.pos(), robot.pos())  # self-transmit (placeholder)
            if not ok:
                # action dropped -> act with zero or previous small jitter
                infos['wifi'].append((i, 'dropped'))
                v_cmd, w_cmd = 0.0, 0.0
            else:
                infos['wifi'].append((i, 'ok'))
            robot.step(v_cmd, w_cmd, self.cfg.dt, noise=self.cfg.domain_randomization)

        # collisions and penalties
        reward = 0.0
        done = False
        # collision robot-robot
        for i in range(len(self.robots)):
            for j in range(i+1, len(self.robots)):
                d = math.hypot(self.robots[i].x - self.robots[j].x, self.robots[i].y - self.robots[j].y)
                if d < 2 * self.cfg.robot_radius:
                    reward += self.cfg.collision_penalty
        # collision with walls
        for r in self.robots:
            if r.x < 0 or r.x > self.cfg.world_w or r.y < 0 or r.y > self.cfg.world_h:
                reward += self.cfg.wall_penalty
        # collision with obstacles
        for r in self.robots:
            for ob in self.obstacles:
                if ob.contains(r.x, r.y):
                    reward += self.cfg.collision_penalty
        # goal reward (distance to goal reduced)
        for i, r in enumerate(self.robots):
            gx, gy = self.goals[i]
            prev_dist = math.hypot(gx - (r.x - r.v*math.cos(r.theta)*self.cfg.dt), gy - (r.y - r.v*math.sin(r.theta)*self.cfg.dt))
            cur_dist = math.hypot(gx - r.x, gy - r.y)
            reward += (prev_dist - cur_dist) * 0.1
            # reaching goal
            if cur_dist < self.cfg.robot_radius*2:
                reward += self.cfg.goal_reward
                done = True

        obs = self._get_obs()
        info = infos
        return obs, float(reward), bool(done), info

    def _get_obs(self):
        obs = []
        for r in self.robots:
            robot_vec = [r.x, r.y, r.theta, r.v]
            lidar = self._compute_lidar(r)
            robot_vec.extend(lidar)
            obs.extend(robot_vec)
        return np.array(obs, dtype=np.float32)

    def _compute_lidar(self, robot: Robot) -> List[float]:
        beams = self.cfg.lidar_beams
        angles = [robot.theta + 2*math.pi*i/beams for i in range(beams)]
        dists = []
        for a in angles:
            dist = self._raycast(robot.x, robot.y, a, self.cfg.lidar_range)
            # add sensor noise
            if self.cfg.domain_randomization:
                dist += random.gauss(0, 2.0)
            dists.append(float(max(0.0, min(self.cfg.lidar_range, dist))))
        return dists

    def _raycast(self, x, y, angle, max_range) -> float:
        # step along ray
        step = 4.0
        t = 0.0
        while t < max_range:
            px = x + math.cos(angle) * t
            py = y + math.sin(angle) * t
            # check bounds
            if px < 0 or px > self.cfg.world_w or py < 0 or py > self.cfg.world_h:
                return t
            # check obstacles
            for ob in self.obstacles:
                if ob.contains(px, py):
                    return t
            t += step
        return max_range

    # ------------------ Rendering ------------------
    def render(self, mode='human'):
        if pygame is None:
            raise RuntimeError('pygame not installed - cannot render')
        # ensure pygame is initialized
        init_pygame_if_needed()
        if self.screen is None:
            self.screen = pygame.display.set_mode((self.cfg.world_w, self.cfg.world_h))
            pygame.display.set_caption('Swarm Simulator')
            self.clock = pygame.time.Clock()
        # draw
        self.screen.fill((245, 245, 245))
        # obstacles
        for ob in self.obstacles:
            pygame.draw.rect(self.screen, (160, 160, 160), pygame.Rect(ob.x, ob.y, ob.w, ob.h))
        # goals
        for g in self.goals:
            pygame.draw.circle(self.screen, (0, 200, 0), (int(g[0]), int(g[1])), 8)
        # robots
        for r in self.robots:
            pygame.draw.circle(self.screen, (50, 100, 220), (int(r.x), int(r.y)), int(self.cfg.robot_radius))
            # heading
            hx = r.x + math.cos(r.theta) * self.cfg.robot_radius
            hy = r.y + math.sin(r.theta) * self.cfg.robot_radius
            pygame.draw.line(self.screen, (0,0,0), (r.x, r.y), (hx, hy), 2)
            # lidar beams
            beams = self._compute_lidar(r)
            angles = [r.theta + 2*math.pi*i/len(beams) for i in range(len(beams))]
            for d, a in zip(beams, angles):
                ex = r.x + math.cos(a) * d
                ey = r.y + math.sin(a) * d
                pygame.draw.aaline(self.screen, (200,200,200), (r.x, r.y), (ex, ey))
        pygame.display.flip()
        # protect against division by zero if dt is 0
        if self.cfg.dt > 0:
            self.clock.tick(max(1, int(1.0 / self.cfg.dt)))

    def close(self):
        if pygame:
            pygame.quit()

# ----------------------------- Utilities & Demo ---------------------------------

def random_policy_action(env: SwarmEnv):
    # uniform random actions within bounds
    acts = []
    for _ in range(env.cfg.n_robots):
        v = random.uniform(-env.cfg.max_speed/2, env.cfg.max_speed/2)
        w = random.uniform(-env.cfg.max_omega/2, env.cfg.max_omega/2)
        acts.extend([v, w])
    return np.array(acts, dtype=np.float32)


def run_demo(steps=1000, render=True, headless=False):
    cfg = Config()
    env = SwarmEnv(cfg)
    obs = env.reset()
    total_reward = 0.0

    # If rendering requested, initialize pygame and the display before calling pygame.event.get()
    if render and not headless:
        if pygame is None:
            print("Pygame not installed; cannot render. Run without --mode demo or install pygame.")
            return
        init_pygame_if_needed()
        # create initial screen (render once to initialize screen & clock)
        try:
            env.render()
        except Exception as e:
            print("Failed to initialize rendering:", e)
            return

    for t in range(steps):
        act = random_policy_action(env)
        obs, r, done, info = env.step(act)
        total_reward += r
        if render and not headless:
            # handle quit events (safe because pygame was initialized above)
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    env.close()
                    return
            env.render()
        if done:
            print(f"Episode finished at t={t}, reward={total_reward:.2f}")
            break
    env.close()
    print('Total reward', total_reward)

# ----------------------------- Example: Minimal training loop -----------------
# This is a minimal on-policy loop (not optimized) that collects trajectories and updates
# a tiny PyTorch policy if user wants to use it. We keep it optional to avoid heavy deps.

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--mode', choices=['demo', 'random', 'train_sb3'], default='demo')
    parser.add_argument('--steps', type=int, default=2000)
    parser.add_argument('--headless', action='store_true')
    args = parser.parse_args()

    if args.mode == 'demo' or args.mode == 'random':
        run_demo(steps=args.steps, render=(args.mode=='demo'), headless=args.headless)
    elif args.mode == 'train_sb3':
        # attempt to use stable-baselines3 PPO; user may install it
        try:
            from stable_baselines3 import PPO
            from stable_baselines3.common.vec_env import DummyVecEnv
            # wrapper env to provide gym.Env
            def make_env():
                return SwarmEnv(Config())
            ve = DummyVecEnv([make_env])
            model = PPO('MlpPolicy', ve, verbose=1)
            model.learn(total_timesteps=args.steps)
            print('Training done')
        except Exception as e:
            print('Failed to run train_sb3. Please install stable-baselines3 and extras. Error:', e)
